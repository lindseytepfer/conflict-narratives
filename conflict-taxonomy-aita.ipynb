{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a24ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f2c31",
   "metadata": {},
   "source": [
    "#### Getting started with PRAW\n",
    "Before praw can be used to scrape data, it needs us to authenticate ourselves. To do this, we need to create a read-only overall Reddit 'instance' and provide it with three pieces of information: your client_id , client_secret, and user_agent (plus your username and password)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1a014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a (general) reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"****\",\n",
    "    client_secret=\"****\",\n",
    "    user_agent=\"****\",\n",
    "    username='****',\n",
    "    password='****!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e2250",
   "metadata": {},
   "source": [
    "### Creating a Subreddit 'Instance'\n",
    "\n",
    "To begin, we must create a subreddit 'instance' (similar to when we created the more general Reddit 'instance') by defining which subreddit we want to scrape. Here, I only care about the AITA subreddit, so I'll create a single instance named `aita`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450531b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating my subreddit \"instance\"\n",
    "aita = reddit.subreddit('AmItheAsshole')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b437ed5",
   "metadata": {},
   "source": [
    "### Before you Scrape\n",
    "\n",
    "1. **Be aware of limits**: Although you can set `limit=None`, the maximum is actually 1000, which is a limitation set by Reddit directly. According to PRAW, you can try to get more than 1,000 by using the search function. \n",
    "\n",
    "2. **Pick your sub-category**: For each Subreddit, different categories of posts can be selectively scraped: best, hot, new, controversial, top, and rising. Here, I am only interested in collecting two categories that we will scrape on a monthly basis: new and top.\n",
    "\n",
    "3. **Prepare for inconsistencies**: PRAW docs notes that you may see a discrepancy between what praw returns and what is actually viewable on the reddit page, because it counts deleted, removed, and spam comments.\n",
    "\n",
    "4. **Data Handling**: If you're saving your data in a as a .JSON object, be sure to cast your \"Redditor\" objects as strings (e.g., do str(comment.author) for example) because otherwise, JSON doesn't know what that object is and it'll break, losing all that precious scraped data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13455800",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path ='/Users/f004p74/Documents/dartmouth/projects/conflict-taxonomy/aita-posts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dc06c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aita_top = aita.top(limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0496276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we ignore the AutoModerator because it's unreleated to the post's content\n",
    "skip_list = ['AutoModerator']\n",
    "\n",
    "for submission in aita_top:\n",
    "    praw_dict = {}\n",
    "    submission_id = submission.id\n",
    "    \n",
    "    praw_dict[\"Submission\"] = {'Title': submission.title,\n",
    "                                'Sub ID': submission.id,\n",
    "                               'URL': submission.url,\n",
    "                              'Body': submission.selftext}\n",
    "    \n",
    "    with open(out_path+submission_id+'_top.json', 'w') as fp:\n",
    "        json.dump(praw_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73c30645",
   "metadata": {},
   "outputs": [],
   "source": [
    "aita_hot = aita.hot(limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "315ac9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we ignore the AutoModerator because it's unreleated to the post's content\n",
    "skip_list = ['AutoModerator']\n",
    "\n",
    "for submission in aita_hot:\n",
    "    praw_dict = {}\n",
    "    submission_id = submission.id\n",
    "    \n",
    "    praw_dict[\"Submission\"] = {'Title': submission.title,\n",
    "                                'Sub ID': submission.id,\n",
    "                               'URL': submission.url,\n",
    "                              'Body': submission.selftext}\n",
    "    \n",
    "    with open(out_path+submission_id+'_hot.json', 'w') as fp:\n",
    "        json.dump(praw_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9b0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ce876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for file in file_list:\n",
    "    try:\n",
    "        with open(out_path+file) as f:\n",
    "            json_dict = json.load(f)\n",
    "\n",
    "        data.append(json_dict[\"Submission\"]['Body'])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39301555",
   "metadata": {},
   "source": [
    "### Text Processing & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e375d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/f004p74/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/f004p74/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/f004p74/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f50333",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\"how'll\": \"how will\",\"how's\": \"how is\",\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\"mightn't\": \"might not\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\"oughtn't\": \"ought not\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\"she'll\": \"she will\",\"she's\": \"she is\",\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\"that'd\": \"that would\",\"that's\": \"that is\",\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\"they'd\": \"they would\",\"they'll\": \"they will\",\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'll\": \"we will\",\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\"what've\": \"what have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\"you're\": \"you are\"}\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.extend(['aita', 'asshole', 'reddit','subreddit','aitah','post',\n",
    "                       'poster','link','original','lurker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f8302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = []\n",
    "\n",
    "for i in data:\n",
    "    text = i.lower() # convert all text to lowercase\n",
    "    text = text.split() #separates the block of text to individual words\n",
    "    new_text = []\n",
    "    for word in text: # converts contractions to separate words\n",
    "        if word in contractions:\n",
    "            new_text.append(contractions[word])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    \n",
    "    \n",
    "    text = \" \".join(new_text)\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\\\', ' ', text)\n",
    "    #text = re.sub(r'\\'', ' ', text) \n",
    "\n",
    "    # Tokenize each word\n",
    "    text = nltk.WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "    # Lemmatize each word\n",
    "    text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='v') for token in text if len(token)>1]\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = [x for x in text if x not in stopwords_list]\n",
    "    \n",
    "    # Convert the list back into a string.\n",
    "    text = ' '.join(map(str, text))\n",
    "    \n",
    "    clean_list.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e5633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in clean_list:\n",
    "    separated = i.split()\n",
    "    for word in separated:\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b146f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating out different categories:\n",
    "\n",
    "#social labels\n",
    "relationships = ['family','mom','dad','grandma','grandpa','grandmother','grandfather',\n",
    "                'aunt','uncle','cousin','sister','brother','girlfriend','boyfriend',\n",
    "                'fiancee','husband','wife','partner','colleague','boss','manager','coworker',\n",
    "                'teammate','classmate','mother','father','daughter','son','baby','child',\n",
    "                'parent','kid','friend','buddy']\n",
    "\n",
    "#emotion words\n",
    "states = pd.read_csv(\"/Users/f004p74/Documents/dartmouth/projects/c-tom-reddit/mental_states.csv\")\n",
    "traits = pd.read_csv(\"/Users/f004p74/Documents/dartmouth/projects/c-tom-reddit/mental_traits.csv\")\n",
    "\n",
    "state_list = list(states[\"States\"])\n",
    "trait_list = list(traits[\"Traits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50a201",
   "metadata": {},
   "source": [
    "### word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15cba83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('say', 5220),\n",
       " ('get', 4823),\n",
       " ('tell', 3905),\n",
       " ('go', 3883),\n",
       " ('would', 2776),\n",
       " ('want', 2764),\n",
       " ('like', 2647),\n",
       " ('make', 2372),\n",
       " ('ask', 2316),\n",
       " ('time', 2152),\n",
       " ('know', 2080),\n",
       " ('think', 1900),\n",
       " ('take', 1830),\n",
       " ('one', 1678),\n",
       " ('call', 1653),\n",
       " ('come', 1634),\n",
       " ('family', 1610),\n",
       " ('work', 1461),\n",
       " ('mom', 1417),\n",
       " ('start', 1389),\n",
       " ('could', 1377),\n",
       " ('leave', 1371),\n",
       " ('even', 1306),\n",
       " ('really', 1279),\n",
       " ('try', 1278),\n",
       " ('kid', 1277),\n",
       " ('back', 1263),\n",
       " ('feel', 1239),\n",
       " ('husband', 1214),\n",
       " ('parent', 1199),\n",
       " ('since', 1195),\n",
       " ('talk', 1169),\n",
       " ('give', 1165),\n",
       " ('sister', 1159),\n",
       " ('also', 1158),\n",
       " ('home', 1110),\n",
       " ('wife', 1072),\n",
       " ('see', 1071),\n",
       " ('us', 1066),\n",
       " ('house', 1061),\n",
       " ('need', 1031),\n",
       " ('years', 979),\n",
       " ('never', 956),\n",
       " ('live', 955),\n",
       " ('people', 942),\n",
       " ('keep', 935),\n",
       " ('day', 920),\n",
       " ('friends', 916),\n",
       " ('dad', 916),\n",
       " ('pay', 903),\n",
       " ('look', 897),\n",
       " ('still', 892),\n",
       " ('daughter', 873),\n",
       " ('friend', 851),\n",
       " ('things', 836),\n",
       " ('year', 826),\n",
       " ('much', 810),\n",
       " ('well', 805),\n",
       " ('brother', 803),\n",
       " ('son', 775),\n",
       " ('move', 768),\n",
       " ('last', 766),\n",
       " ('find', 751),\n",
       " ('money', 746),\n",
       " ('help', 740),\n",
       " (').', 737),\n",
       " ('ago', 733),\n",
       " ('use', 732),\n",
       " ('first', 728),\n",
       " ('let', 718),\n",
       " ('something', 699),\n",
       " ('way', 698),\n",
       " ('always', 685),\n",
       " ('wed', 683),\n",
       " ('lot', 668),\n",
       " ('stay', 642),\n",
       " ('around', 642),\n",
       " ('two', 638),\n",
       " ('mother', 630),\n",
       " ('room', 625),\n",
       " ('everyone', 620),\n",
       " ('anything', 608),\n",
       " ('week', 603),\n",
       " ('right', 598),\n",
       " ('school', 584),\n",
       " ('put', 581),\n",
       " ('months', 578),\n",
       " ('love', 567),\n",
       " ('good', 561),\n",
       " ('life', 554),\n",
       " ('decide', 549),\n",
       " ('stop', 537),\n",
       " ('job', 526),\n",
       " ('thing', 522),\n",
       " ('happen', 520),\n",
       " ('edit', 512),\n",
       " ('away', 510),\n",
       " ('old', 506),\n",
       " ('baby', 506),\n",
       " ('night', 503)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(all_words).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9293b667",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpora\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcorpora\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m id2word \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241m.\u001b[39mcorpora\u001b[38;5;241m.\u001b[39mDictionary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = gensim.corpora.Dictionary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2402bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus: Term Document Frequency\n",
    "dataset = [d.split() for d in clean_list]\n",
    "id2word = gensim.corpora.Dictionary(dataset)\n",
    "\n",
    "corpus = []\n",
    "for text in dataset:\n",
    "    new_text = id2word.doc2bow(text)\n",
    "    corpus.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90a218cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           iterations=50,\n",
    "                                           num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7848d58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgensim_models\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n\u001b[1;32m      3\u001b[0m vis \u001b[38;5;241m=\u001b[39m pyLDAvis\u001b[38;5;241m.\u001b[39mgensim_models\u001b[38;5;241m.\u001b[39mprepare(lda_model, corpus, dictionary\u001b[38;5;241m=\u001b[39mlda_model\u001b[38;5;241m.\u001b[39mid2word)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "976d52bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m      7\u001b[0m     lda_model \u001b[38;5;241m=\u001b[39m gensim\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mldamodel\u001b[38;5;241m.\u001b[39mLdaModel(corpus\u001b[38;5;241m=\u001b[39mcorpus,\n\u001b[1;32m      8\u001b[0m                                            id2word\u001b[38;5;241m=\u001b[39mid2word,\n\u001b[1;32m      9\u001b[0m                                            iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                            num_topics\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m---> 11\u001b[0m     coherence_model_lda \u001b[38;5;241m=\u001b[39m CoherenceModel(model\u001b[38;5;241m=\u001b[39mlda_model, texts\u001b[38;5;241m=\u001b[39m\u001b[43mreviews\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview_Clean_List\u001b[39m\u001b[38;5;124m'\u001b[39m], dictionary\u001b[38;5;241m=\u001b[39mid2word, coherence\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_v\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m     coherence_lda \u001b[38;5;241m=\u001b[39m coherence_model_lda\u001b[38;5;241m.\u001b[39mget_coherence()\n\u001b[1;32m     13\u001b[0m     number_of_topics\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "number_of_topics = []\n",
    "coherence_score = []\n",
    "for i in range(1,6):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           iterations=50,\n",
    "                                           num_topics=i)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=reviews['Review_Clean_List'], dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    number_of_topics.append(i)\n",
    "    coherence_score.append(coherence_lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6352bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e7d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf2414a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d9170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e710dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0cabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyLDAvis\n",
    "# import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-praw-env",
   "language": "python",
   "name": "reddit-praw-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
