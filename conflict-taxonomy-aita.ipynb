{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a24ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 14:09:06.787650: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "#language models\n",
    "import nltk\n",
    "import datawrangler as dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1615f12a",
   "metadata": {},
   "source": [
    "#### Getting started with PRAW\n",
    "Before praw can be used to scrape data, it needs us to authenticate ourselves. To do this, we need to create a read-only overall Reddit 'instance' and provide it with three pieces of information: your client_id , client_secret, and user_agent (plus your username and password)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1a014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a (general) reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"****\",\n",
    "    client_secret=\"****\",\n",
    "    user_agent=\"****\",\n",
    "    username='****',\n",
    "    password='****!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b87c88",
   "metadata": {},
   "source": [
    "### Creating a Subreddit 'Instance'\n",
    "\n",
    "To begin, we must create a subreddit 'instance' (similar to when we created the more general Reddit 'instance') by defining which subreddit we want to scrape. Here, I only care about the AITA subreddit, so I'll create a single instance named `aita`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450531b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating my subreddit \"instance\"\n",
    "aita = reddit.subreddit('AmItheAsshole')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443216e",
   "metadata": {},
   "source": [
    "### Before you Scrape\n",
    "\n",
    "1. **Be aware of limits**: Although you can set `limit=None`, the maximum is actually 1000, which is a limitation set by Reddit directly. According to PRAW, you can try to get more than 1,000 by using the search function. \n",
    "\n",
    "2. **Pick your sub-category**: For each Subreddit, different categories of posts can be selectively scraped: best, hot, new, controversial, top, and rising. Here, I am only interested in collecting two categories that we will scrape on a monthly basis: new and top.\n",
    "\n",
    "3. **Prepare for inconsistencies**: PRAW docs notes that you may see a discrepancy between what praw returns and what is actually viewable on the reddit page, because it counts deleted, removed, and spam comments.\n",
    "\n",
    "4. **Data Handling**: If you're saving your data in a as a .JSON object, be sure to cast your \"Redditor\" objects as strings (e.g., do str(comment.author) for example) because otherwise, JSON doesn't know what that object is and it'll break, losing all that precious scraped data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e16990b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path ='/Users/f004p74/Documents/dartmouth/projects/conflict-taxonomy/aita-posts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6940e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aita_top = aita.top(limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c94f694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we ignore the AutoModerator because it's unreleated to the post's content\n",
    "skip_list = ['AutoModerator']\n",
    "\n",
    "for submission in aita_top:\n",
    "    praw_dict = {}\n",
    "    submission_id = submission.id\n",
    "    \n",
    "    praw_dict[\"Submission\"] = {'Title': submission.title,\n",
    "                                'Sub ID': submission.id,\n",
    "                               'URL': submission.url,\n",
    "                              'Body': submission.selftext}\n",
    "    \n",
    "    with open(out_path+submission_id+'_top.json', 'w') as fp:\n",
    "        json.dump(praw_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c42e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "aita_hot = aita.hot(limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "247370e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we ignore the AutoModerator because it's unreleated to the post's content\n",
    "skip_list = ['AutoModerator']\n",
    "\n",
    "for submission in aita_hot:\n",
    "    praw_dict = {}\n",
    "    submission_id = submission.id\n",
    "    \n",
    "    praw_dict[\"Submission\"] = {'Title': submission.title,\n",
    "                                'Sub ID': submission.id,\n",
    "                               'URL': submission.url,\n",
    "                              'Body': submission.selftext}\n",
    "    \n",
    "    with open(out_path+submission_id+'_hot.json', 'w') as fp:\n",
    "        json.dump(praw_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f238fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3070c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for file in file_list:\n",
    "    try:\n",
    "        with open(out_path+file) as f:\n",
    "            json_dict = json.load(f)\n",
    "\n",
    "        data.append(json_dict[\"Submission\"]['Body'])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18400555",
   "metadata": {},
   "source": [
    "### Text Processing & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ce54b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/f004p74/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/f004p74/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/f004p74/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8b4c748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\"how'll\": \"how will\",\"how's\": \"how is\",\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\"mightn't\": \"might not\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\"oughtn't\": \"ought not\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\"she'll\": \"she will\",\"she's\": \"she is\",\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\"that'd\": \"that would\",\"that's\": \"that is\",\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\"they'd\": \"they would\",\"they'll\": \"they will\",\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'll\": \"we will\",\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\"what've\": \"what have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\"you're\": \"you are\"}\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.extend(['aita', 'asshole', 'reddit','subreddit','aitah','post',\n",
    "                       'poster','link','original','lurker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "916aeb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = []\n",
    "\n",
    "for i in data:\n",
    "    text = i.lower() # convert all text to lowercase\n",
    "    text = text.split() #separates the block of text to individual words\n",
    "    new_text = []\n",
    "    for word in text: # converts contractions to separate words\n",
    "        if word in contractions:\n",
    "            new_text.append(contractions[word])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    \n",
    "    \n",
    "    text = \" \".join(new_text)\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\\\', ' ', text)\n",
    "    #text = re.sub(r'\\'', ' ', text) \n",
    "\n",
    "    # Tokenize each word\n",
    "    text = nltk.WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "    # Lemmatize each word\n",
    "    text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='v') for token in text if len(token)>1]\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = [x for x in text if x not in stopwords_list]\n",
    "    \n",
    "    # Convert the list back into a string.\n",
    "    text = ' '.join(map(str, text))\n",
    "    \n",
    "    clean_list.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a2a81e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in clean_list:\n",
    "    separated = i.split()\n",
    "    for word in separated:\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c682f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating out different categories:\n",
    "\n",
    "#social labels\n",
    "\n",
    "#emotion words\n",
    "states = pd.read_csv(\"/Users/f004p74/Documents/dartmouth/projects/c-tom-reddit/mental_states.csv\")\n",
    "traits = pd.read_csv(\"/Users/f004p74/Documents/dartmouth/projects/c-tom-reddit/mental_traits.csv\")\n",
    "\n",
    "state_list = list(states[\"States\"])\n",
    "trait_list = list(traits[\"Traits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0dae86",
   "metadata": {},
   "source": [
    "### word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a4a2372e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('say', 5220), ('get', 4823), ('tell', 3905), ('go', 3883), ('would', 2776)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(all_words).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07d277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b6f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557287a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14d6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77082966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff150722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431c923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7286656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-praw-env",
   "language": "python",
   "name": "reddit-praw-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
